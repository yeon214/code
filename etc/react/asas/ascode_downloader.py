import os
import time
import requests
from bs4 import BeautifulSoup

# === 1. ë³µì‚¬í•œ ì¿ í‚¤ ì…ë ¥ ===
cookies = {
    "PHPSESSID": "j3pvppb7cnu5m7la17qsdd3ha7",
    "lastlang": "undefined"
}

# === 2. ê¸°ë³¸ ì„¤ì • ===
save_root = "./ascode_solutions"
base_url = "http://ascode.org"
status_url = f"{base_url}/status.php"
username = "20243077"  # ë³¸ì¸ ì‚¬ìš©ì ID

# === 3. ì„¸ì…˜ ìƒì„± ë° í—¤ë”/ì¿ í‚¤ ì ìš© ===
session = requests.Session()
session.cookies.update(cookies)
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36',
    'Referer': 'http://ascode.org/modifypage.php',
    'Accept': '*/*',
    'Accept-Encoding': 'gzip, deflate',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
})

# === 4. ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ===
def check_login_status():
    try:
        check_url = "http://ascode.org/template/ascode/profile.php?138760013"
        resp = session.get(check_url)
        with open('debug_profile.html', 'w', encoding='utf-8') as f:
            f.write(resp.text)
        soup = BeautifulSoup(resp.text, 'html.parser')
        logout_link = soup.find('a', string='Logout')
        return logout_link is not None
    except Exception as e:
        print("ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜:", e)
        return False

# === 5. ì œì¶œ ê¸°ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ===
def fetch_user_submissions(user_id, top=None, prevtop=None, result="4"):
    # URLì— 'top'ê³¼ 'prevtop'ì„ í¬í•¨í•˜ì—¬ í˜ì´ì§€ë¥¼ ìš”ì²­
    url = f"{status_url}?user_id={user_id}&jresult={result}"
    if top:
        url += f"&top={top}"
    if prevtop:
        url += f"&prevtop={prevtop}"
    
    try:
        resp = session.get(url)
        resp.raise_for_status()
        return resp.text
    except requests.RequestException as e:
        print(f"ì œì¶œ ê¸°ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return None

# === 6. ì œì¶œ ì½”ë“œ ì €ì¥ ===
def save_code_from_page(user_id, top=None, prevtop=None):
    page_html = fetch_user_submissions(user_id, top, prevtop, result="4")  # ACë§Œ ê°€ì ¸ì˜¤ê¸°
    if not page_html:
        return False

    soup = BeautifulSoup(page_html, 'html.parser')
    table = soup.find('table')
    if not table:
        print("ì œì¶œ ê¸°ë¡ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False

    headers = [th.get_text(strip=True).lower() for th in table.find('tr').find_all('th')]
    try:
        runid_idx = headers.index('runid')
        problem_idx = headers.index('problem')
        lang_idx = headers.index('language')
    except ValueError as e:
        print(f"í•„ìš”í•œ ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print(f"ë°œê²¬ëœ í—¤ë”: {headers}")
        return False

    submissions = []
    for row in table.find_all('tr')[1:]:
        cols = row.find_all('td')
        if len(cols) <= max(runid_idx, problem_idx, lang_idx):
            continue
        runid = cols[runid_idx].get_text(strip=True)
        problem_id = cols[problem_idx].get_text(strip=True)
        language_cell = cols[lang_idx]
        language_text = language_cell.get_text(strip=True)

        # Edit ë§í¬ ì°¾ê¸°
        edit_link = None
        for a_tag in language_cell.find_all('a'):
            if 'edit' in a_tag.get_text().lower():
                edit_link = a_tag.get('href')
                break
        if not edit_link:
            edit_link = f"showsource.php?id={runid}"
        if edit_link.startswith('/'):
            edit_link = f"{base_url}{edit_link}"
        elif not edit_link.startswith('http'):
            edit_link = f"{base_url}/{edit_link}"

        if '/' in language_text:
            language = language_text.split('/')[0]
        else:
            language = language_text.replace('Edit', '').strip()

        submissions.append((runid, problem_id, language, edit_link))

    for idx, (runid, problem_id, language, source_link) in enumerate(submissions):
        print(f"[{idx+1}/{len(submissions)}] RunID {runid}, ë¬¸ì œ {problem_id}, ì–¸ì–´ {language} ì²˜ë¦¬ ì¤‘...")
        try:
            resp = session.get(source_link)
            resp.raise_for_status()
        except requests.RequestException as e:
            print(f"[{problem_id}] ì½”ë“œ ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue

        soup = BeautifulSoup(resp.text, 'html.parser')
        code_element = soup.find('pre') or soup.find('textarea')
        if code_element:
            code_text = code_element.get_text()
            extension = get_file_extension(language)
            problem_folder = os.path.join(save_root, problem_id)
            os.makedirs(problem_folder, exist_ok=True)
            file_path = os.path.join(problem_folder, f"solution_{runid}{extension}")
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(code_text)
            print(f"[{problem_id}] ì½”ë“œ(RunID: {runid}) ì €ì¥ ì™„ë£Œ: {file_path}")
        else:
            print(f"[{problem_id}] ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        time.sleep(1)
    return True

# === 7. íŒŒì¼ í™•ì¥ì ë§¤ì¹­ ===
def get_file_extension(language):
    language = language.lower()
    if "c++" in language or "cpp" in language:
        return ".cpp"
    elif "c" in language and "c++" not in language:
        return ".c"
    elif "java" in language:
        return ".java"
    elif "python" in language or "py" in language:
        return ".py"
    elif "javascript" in language or "js" in language:
        return ".js"
    else:
        return ".txt"

# === 8. í˜ì´ì§€ ë‚´ë¹„ê²Œì´ì…˜ ì²˜ë¦¬ ===
def get_next_page_top(soup):
    next_page_link = soup.find('a', string='Next Page')
    if next_page_link:
        # 'Next Page' ë§í¬ì—ì„œ topê³¼ prevtop ì¶”ì¶œ
        next_url = next_page_link.get('href')
        top = next_url.split('top=')[1].split('&')[0]
        prevtop = next_url.split('prevtop=')[1] if 'prevtop' in next_url else None
        return top, prevtop
    return None, None

# === 9. ë©”ì¸ ===
def main():
    if not os.path.exists(save_root):
        os.makedirs(save_root)
    if not check_login_status():
        print("âŒ ì¿ í‚¤ê°€ ì˜¬ë°”ë¥´ì§€ ì•Šê±°ë‚˜ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì €ì—ì„œ ì¿ í‚¤ë¥¼ ìƒˆë¡œ ë³µì‚¬í•˜ì„¸ìš”.")
        return
    print(f"âœ… AScode.orgì— ë¡œê·¸ì¸ëœ ìƒíƒœì…ë‹ˆë‹¤. ì‚¬ìš©ì: {username}")
    page = 1
    max_pages = 20
    top = None
    prevtop = None

    while page <= max_pages:
        print(f"====== í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹œì‘ ======")
        if not save_code_from_page(username, top, prevtop):
            print(f"í˜ì´ì§€ {page}ì—ì„œ ì²˜ë¦¬ ì¤‘ë‹¨ - ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ")
            break

        page_html = fetch_user_submissions(username, top, prevtop)
        if page_html:
            soup = BeautifulSoup(page_html, 'html.parser')
            # í˜ì´ì§€ì˜ 'Next Page' ë§í¬ë¥¼ ì°¾ì•„ topê³¼ prevtopì„ ì¶”ì¶œ
            top, prevtop = get_next_page_top(soup)
            if top:
                page += 1
                continue
        break
    print("ğŸ‰ ëª¨ë“  ì½”ë“œ ë‹¤ìš´ë¡œë“œ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ë‹¤ìš´ë¡œë“œëœ ì½”ë“œëŠ” {save_root} í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()